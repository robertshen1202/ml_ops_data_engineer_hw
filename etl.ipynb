{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess and Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_parquet(\"data/sample.parquet\")\n",
    "\n",
    "# remove rows any null value\n",
    "sample_df = sample_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check time columns\n",
    "\n",
    "The time string is processed here. It seems that it is some variant of ISO8601, but for some cases the second decimals are not zeropadded with a fix length. A regex match is ran to ensure there is no outlier format. \n",
    "\n",
    "An potential improvement is to stored time in epoch_ms from the start. There is no point for time to be human readable at the point, so it is much better to store time simply as epoch in milliseconds. This can improve data integrity check, remove string format ambiguity, and sorting and storing efficency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only acceptable time string format\n",
    "time_regex = r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z\"\n",
    "sample_df = sample_df.loc[sample_df[\"time\"].str.match(time_regex)]\n",
    "\n",
    "# convert time column to epoch_ms\n",
    "sample_df[\"time\"] = pd.to_datetime(sample_df['time'], format = \"mixed\")\n",
    "sample_df['time'] = (sample_df['time'].astype(np.int64) // 10**6).astype(np.int64)\n",
    "\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep only acceptable value for the rest of the columns\n",
    "\n",
    "the accepted value is kept in a dict. For each columns names, it has to have a \"dtype\" attribute, and can have an optional list of \"accepted_values\"\n",
    "\n",
    "Ex: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"col_name_1\": {\n",
    "        \"dtype\": \"dtype\", \n",
    "        \"accepted_values\": [\n",
    "            \"accpeted_value_1\",\n",
    "            \"accpeted_value_2\",\n",
    "            ...\n",
    "            \"accepted_value_x\"\n",
    "        ]\n",
    "    }...\n",
    "}\n",
    "```\n",
    "\n",
    "For any additional columns in the future, simply adding them to the dictionary. Additional, the dictionary can be store as a json config file for easy reading and editing. \n",
    "\n",
    "robot_id is converted to a string becuase later it needs to be transposed to a column names. For other use cases, it is best to keep it an int. \n",
    "\n",
    "I am not too sure why run_uuid is stored in a float64 instead of int64. Kept it same as in sample.parquet\n",
    "\n",
    "An potential improvement is for any columns with a set of values, such as field and sensor_type, have an additional columns that stored them in an integer as instead. Ex. add an column called \"sensor_type_int\", and store \"encoder\" as 0 and \"load_cell\" as 1. Having a int column can improve filtering and sort immersive comparing to a string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_configs = {\n",
    "    \"value\": {\n",
    "        \"dtype\": \"float64\"\n",
    "    },\n",
    "    \"field\": {\n",
    "        \"dtype\": \"string\", \n",
    "        \"accepted_values\": [\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"z\",\n",
    "            \"fx\",\n",
    "            \"fy\",\n",
    "            \"fz\",\n",
    "        ],\n",
    "    },\n",
    "    \"robot_id\": {\n",
    "        \"dtype\": \"string\", \n",
    "    }, \n",
    "    \"run_uuid\": {\n",
    "        \"dtype\": \"float64\", \n",
    "    },\n",
    "    \"sensor_type\": {\n",
    "        \"dtype\": \"string\", \n",
    "        \"accepted_values\": [\n",
    "            \"encoder\", \n",
    "            \"load_cell\",\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "\n",
    "# go through the col and only keep acceptable list\n",
    "for col_name, config in col_configs.items(): \n",
    "    # set col dtype\n",
    "    sample_df[col_name] = sample_df[col_name].astype(config[\"dtype\"])\n",
    "\n",
    "    # if the columns has a list of acceptable value, enforce it\n",
    "    if \"accepted_values\" in col_configs[col_name]: \n",
    "        sample_df = sample_df.loc[sample_df[col_name].isin(config[\"accepted_values\"])]\n",
    "\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and sort columns to improve filtering and searching\n",
    "\n",
    "The specific columns to index can be adjusted depending on user case. For this specific workload, i chose to index all columns except value to improve searching time during pviot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df\\\n",
    "    .set_index([\"run_uuid\", \"robot_id\", \"sensor_type\", \"field\", \"time\"])\\\n",
    "    .sort_index()\n",
    "\n",
    "display(sample_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
