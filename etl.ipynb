{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess and Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_parquet(\"data/sample.parquet\")\n",
    "\n",
    "# remove rows any null value\n",
    "sample_df = sample_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check time columns\n",
    "\n",
    "The time string is processed here. It seems that it is some variant of ISO8601, but for some cases the second decimals are not zeropadded with a fix length. A regex match is ran to ensure there is no outlier format. \n",
    "\n",
    "An potential improvement is to stored time in epoch_ms from the start. There is no point for time to be human readable at the point, so it is much better to store time simply as epoch in milliseconds. This can improve data integrity check, remove string format ambiguity, and sorting and storing efficency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only acceptable time string format\n",
    "time_regex = r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z\"\n",
    "sample_df = sample_df.loc[sample_df[\"time\"].str.match(time_regex)]\n",
    "\n",
    "# convert time column to epoch_ms\n",
    "sample_df[\"time\"] = pd.to_datetime(sample_df['time'], format = \"mixed\")\n",
    "sample_df['time'] = (sample_df['time'].astype(np.int64) // 10**6).astype(np.int64)\n",
    "\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep only acceptable value for the rest of the columns\n",
    "\n",
    "the accepted value is kept in a dict. For each columns names, it has to have a \"dtype\" attribute, and can have an optional list of \"accepted_values\"\n",
    "\n",
    "Ex: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"col_name_1\": {\n",
    "        \"dtype\": \"dtype\", \n",
    "        \"accepted_values\": [\n",
    "            \"accpeted_value_1\",\n",
    "            \"accpeted_value_2\",\n",
    "            ...\n",
    "            \"accepted_value_x\"\n",
    "        ]\n",
    "    }...\n",
    "}\n",
    "```\n",
    "\n",
    "For any additional columns in the future, simply adding them to the dictionary. Additional, the dictionary can be store as a json config file for easy reading and editing. \n",
    "\n",
    "robot_id is converted to a string becuase later it needs to be transposed to a column names. For other use cases, it is best to keep it an int. \n",
    "\n",
    "I am not too sure why run_uuid is stored in a float64 instead of int64. Kept it same as in sample.parquet\n",
    "\n",
    "An potential improvement is for any columns with a set of values, such as field and sensor_type, have an additional columns that stored them in an integer as instead. Ex. add an column called \"sensor_type_int\", and store \"encoder\" as 0 and \"load_cell\" as 1. Having a int column can improve filtering and sort immersive comparing to a string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_configs = {\n",
    "    \"value\": {\n",
    "        \"dtype\": \"float64\"\n",
    "    },\n",
    "    \"field\": {\n",
    "        \"dtype\": \"string\", \n",
    "        \"accepted_values\": [\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"z\",\n",
    "            \"fx\",\n",
    "            \"fy\",\n",
    "            \"fz\",\n",
    "        ],\n",
    "    },\n",
    "    \"robot_id\": {\n",
    "        \"dtype\": \"string\", \n",
    "    }, \n",
    "    \"run_uuid\": {\n",
    "        \"dtype\": \"float64\", \n",
    "    },\n",
    "    \"sensor_type\": {\n",
    "        \"dtype\": \"string\", \n",
    "        \"accepted_values\": [\n",
    "            \"encoder\", \n",
    "            \"load_cell\",\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "\n",
    "# go through the col and only keep acceptable list\n",
    "for col_name, config in col_configs.items(): \n",
    "    # set col dtype\n",
    "    sample_df[col_name] = sample_df[col_name].astype(config[\"dtype\"])\n",
    "\n",
    "    # if the columns has a list of acceptable value, enforce it\n",
    "    if \"accepted_values\" in col_configs[col_name]: \n",
    "        sample_df = sample_df.loc[sample_df[col_name].isin(config[\"accepted_values\"])]\n",
    "\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and sort columns to improve filtering and searching\n",
    "\n",
    "The specific columns to index can be adjusted depending on user case. For this specific workload, i chose to index all columns except value to improve searching time during pviot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df\\\n",
    "    .set_index([\"run_uuid\", \"robot_id\", \"sensor_type\", \"field\", \"time\"])\\\n",
    "    .sort_index()\n",
    "\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert timeseries to a wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot table based on field and robot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table to wide format\n",
    "wide_df = pd.pivot_table(\n",
    "    sample_df, \n",
    "    values=\"value\", \n",
    "    index=[\"run_uuid\", \"time\"],\n",
    "    columns=[\"field\", \"robot_id\"],\n",
    ")\n",
    "\n",
    "# concat two levels of columns into one level\n",
    "wide_df.columns = wide_df.columns.map('_'.join)\n",
    "\n",
    "display(wide_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate Missing Value\n",
    "\n",
    "For run_uuid with partial data, pandas interpolate function is used. The function will filling missing data based on previous and next available data point, as well as taking into account the amount of time passed. One downside is that the function assuming everything moves at a constant velocity between each data point. With how small time difference between each measurement is (<10ms), The effort from change in velocity should be minimal. \n",
    "\n",
    "For run_uuid with no value at all, zero is used to filling the blank. The zero make sure the following steps does not break, and it indicates missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to interpolate within each group\n",
    "def interpolate_group(group_df):\n",
    "    col_list = list(group_df.columns)\n",
    "    for col in col_list: \n",
    "        group_df[col] = group_df[col].interpolate(\n",
    "            method = \"time\", \n",
    "            limit_direction = \"both\"\n",
    "        )\n",
    "        \n",
    "    return group_df\n",
    "\n",
    "# set time as the index to prepare for time interpolation\n",
    "interpolate_df = wide_df.reset_index().copy()\n",
    "interpolate_df[\"time\"] = pd.to_datetime(interpolate_df[\"time\"], unit='ms')\n",
    "interpolate_df = interpolate_df.set_index(\"time\")\n",
    "\n",
    "# apply on interpolation function based on groupby run_uuid\n",
    "interpolate_df = interpolate_df\\\n",
    "    .groupby(\"run_uuid\")\\\n",
    "    .apply(interpolate_group, include_groups = False)\n",
    "\n",
    "# fills the remaining NaN value with zero\n",
    "interpolate_df = interpolate_df.fillna(0)\n",
    "\n",
    "interpolate_df = interpolate_df.reset_index()\n",
    "interpolate_df[\"time\"] = interpolate_df[\"time\"].astype(\"int64\") / int(1e6)\n",
    "\n",
    "display(interpolate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Include Engineered/Calculated Features\n",
    "\n",
    "Units: \n",
    "\n",
    "Velcoity = encodor_unit/s\n",
    "\n",
    "Acceleration = encodor_unit/s^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate column names for each measurement\n",
    "p_cols = [] # position columns\n",
    "d_cols = [] # distance columns\n",
    "v_cols = [] # velocity columns\n",
    "a_cols = [] # acceleration columns\n",
    "f_cols = [] # force columns\n",
    "for r_id in [\"1\", \"2\"]: \n",
    "    for axis in [\"x\", \"y\", \"z\"]: \n",
    "        p_cols.append(f\"{axis}_{r_id}\")\n",
    "        d_cols.append(f\"d{axis}_{r_id}\")\n",
    "        v_cols.append(f\"v{axis}_{r_id}\")\n",
    "        a_cols.append(f\"a{axis}_{r_id}\")\n",
    "        f_cols.append(f\"f{axis}_{r_id}\")\n",
    "\n",
    "# sort by time \n",
    "interpolate_df = interpolate_df.sort_values(\"time\", ascending = True)\n",
    "# group by run_uuid\n",
    "interpolate_df_groupby = interpolate_df.groupby(\"run_uuid\")\n",
    "\n",
    "# calculate distance\n",
    "interpolate_df[d_cols] = interpolate_df_groupby[p_cols].diff()\n",
    "\n",
    "# calculate velocity\n",
    "interpolate_df[v_cols] = interpolate_df[d_cols]\\\n",
    "    .div(interpolate_df_groupby[\"time\"].diff() * 1000, axis = 0)\n",
    "\n",
    "# calculate acceleration\n",
    "interpolate_df[a_cols] = interpolate_df_groupby[v_cols].diff()\\\n",
    "    .div(interpolate_df_groupby[\"time\"].diff() * 1000, axis = 0)\n",
    "\n",
    "# loop through two robots\n",
    "for r_id in [\"1\", \"2\"]: \n",
    "    # calculate total distance\n",
    "    interpolate_df[f\"d{r_id}\"] = np.sqrt(\n",
    "        interpolate_df[f\"dx_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"dy_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"dz_{r_id}\"] ** 2\n",
    "    )\n",
    "    # calculate total velocity\n",
    "    interpolate_df[f\"v{r_id}\"] = np.sqrt(\n",
    "        interpolate_df[f\"vx_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"vy_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"vz_{r_id}\"] ** 2\n",
    "    )\n",
    "    # calculate total acceleration\n",
    "    interpolate_df[f\"a{r_id}\"] = np.sqrt(\n",
    "        interpolate_df[f\"ax_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"ay_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"az_{r_id}\"] ** 2\n",
    "    )\n",
    "    # calculate total acceleration\n",
    "    interpolate_df[f\"f{r_id}\"] = np.sqrt(\n",
    "        interpolate_df[f\"fx_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"fy_{r_id}\"] ** 2 + \n",
    "        interpolate_df[f\"fz_{r_id}\"] ** 2\n",
    "    )\n",
    "\n",
    "# add the begining for each run_id, there will be NaN for velocity and acceleration\n",
    "interpolate_df = interpolate_df.fillna(0)\n",
    "\n",
    "display(interpolate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calcualte Runtime Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range function\n",
    "def runtime(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "# groupby run_uuid and aggerate\n",
    "stat_df = interpolate_df.groupby(\"run_uuid\").agg({\n",
    "    \"time\": [\"min\", \"max\", runtime],\n",
    "    \"d1\": [\"sum\"],\n",
    "    \"d2\": [\"sum\"],\n",
    "})\n",
    "\n",
    "# rename columns\n",
    "stat_df.columns = [\n",
    "    \"run_starttime\", \n",
    "    \"run_endtime\", \n",
    "    \"total_runtime_ms\", \n",
    "    \"total_distance_1\", \n",
    "    \"total_distance_2\"\n",
    "]\n",
    "\n",
    "# change epoch to human readable format\n",
    "stat_df[\"run_starttime\"] = pd.to_datetime(stat_df[\"run_starttime\"], unit = \"ms\")\n",
    "stat_df[\"run_endtime\"] = pd.to_datetime(stat_df[\"run_endtime\"], unit = \"ms\")\n",
    "stat_df = stat_df.reset_index()\n",
    "display(stat_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
